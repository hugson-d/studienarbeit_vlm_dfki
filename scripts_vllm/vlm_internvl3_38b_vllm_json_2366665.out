‚úÖ HF_TOKEN geladen
==========================================
üöÄ VLM Benchmark: InternVL3-38B (vLLM + JSON Schema Guided Decoding)
PROJECT_ROOT: /netscratch/dhug/studienarbeit_vlm_dfki
==========================================
üì¶ Erstelle venv und installiere vLLM Dependencies...
‚úÖ Venv erstellt: /netscratch/root/.venv/vllm_internvl
Looking in indexes: http://pypi-cache/index, https://pypi.ngc.nvidia.com
Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.1)
Collecting pip
  Downloading http://pypi-cache/packages/44/3c/d717024885424591d5376220b5e836c2d5293ce2011523c9de23ff7bf068/pip-25.3-py3-none-any.whl.metadata (4.7 kB)
Downloading http://pypi-cache/packages/44/3c/d717024885424591d5376220b5e836c2d5293ce2011523c9de23ff7bf068/pip-25.3-py3-none-any.whl (1.8 MB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.8/1.8 MB 400.1 MB/s eta 0:00:00
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 23.3.1
    Uninstalling pip-23.3.1:
      Successfully uninstalled pip-23.3.1
Successfully installed pip-25.3
Found existing installation: flash-attn 2.0.4
Uninstalling flash-attn-2.0.4:
  Successfully uninstalled flash-attn-2.0.4
‚úÖ Installation abgeschlossen
DEBUG: Python: /usr/bin/python
vLLM Version: 0.12.0
Transformers: 4.57.3
timm: 1.0.22
‚ñ∂Ô∏è Starte InternVL3-38B Evaluation mit vLLM...
‚úÖ .env geladen aus: /netscratch/dhug/studienarbeit_vlm_dfki/.env
‚úÖ HuggingFace Login erfolgreich
‚ÑπÔ∏è GuidedDecodingParams nicht verf√ºgbar - nutze guided_json direkt
INFO 12-06 18:18:52 [utils.py:253] non-default args: {'trust_remote_code': True, 'dtype': 'bfloat16', 'seed': None, 'max_model_len': 4096, 'disable_log_stats': True, 'model': 'OpenGVLab/InternVL3-38B'}
WARNING 12-06 18:18:52 [arg_utils.py:1175] `seed=None` is equivalent to `seed=0` in V1 Engine. You will no longer be allowed to pass `None` in v0.13.
INFO 12-06 18:19:01 [model.py:637] Resolved architecture: InternVLChatModel
INFO 12-06 18:19:01 [model.py:1750] Using max model len 4096
INFO 12-06 18:19:01 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(EngineCore_DP0 pid=708653)[0;0m INFO 12-06 18:19:03 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='OpenGVLab/InternVL3-38B', speculative_config=None, tokenizer='OpenGVLab/InternVL3-38B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=OpenGVLab/InternVL3-38B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=708653)[0;0m INFO 12-06 18:19:04 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.33.195:40531 backend=nccl
[0;36m(EngineCore_DP0 pid=708653)[0;0m INFO 12-06 18:19:04 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=708653)[0;0m WARNING 12-06 18:19:04 [func_utils.py:230] The following intended overrides are not keyword args and will be dropped: {'truncation'}
[0;36m(EngineCore_DP0 pid=708653)[0;0m WARNING 12-06 18:19:04 [func_utils.py:230] The following intended overrides are not keyword args and will be dropped: {'truncation'}
[0;36m(EngineCore_DP0 pid=708653)[0;0m INFO 12-06 18:19:05 [gpu_model_runner.py:3467] Starting to load model OpenGVLab/InternVL3-38B...
[0;36m(EngineCore_DP0 pid=708653)[0;0m INFO 12-06 18:19:05 [layer.py:500] Using AttentionBackendEnum.FLASH_ATTN for MultiHeadAttention in multimodal encoder.
[0;36m(EngineCore_DP0 pid=708653)[0;0m INFO 12-06 18:19:25 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 834, in run_engine_core
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 610, in __init__
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     super().__init__(
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     self._init_executor()
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     self.driver_worker.load_model()
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 3484, in load_model
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     self.model = model_loader.load_model(
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     model = initialize_model(
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/internvl.py", line 1120, in __init__
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     self.language_model = init_vllm_registered_model(
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py", line 359, in init_vllm_registered_model
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     return initialize_model(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py", line 512, in __init__
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     self.model = Qwen2Model(
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/compilation/decorators.py", line 291, in __init__
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     old_init(self, **kwargs)
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py", line 365, in __init__
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     self.start_layer, self.end_layer, self.layers = make_layers(
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py", line 605, in make_layers
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     + [
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py", line 606, in <listcomp>
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py", line 367, in <lambda>
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     lambda prefix: decoder_layer_type(
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py", line 243, in __init__
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     self.mlp = Qwen2MLP(
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py", line 85, in __init__
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     self.gate_up_proj = MergedColumnParallelLinear(
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     super().__init__(
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     self.quant_method.create_weights(
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     data=torch.empty(
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=708653)[0;0m ERROR 12-06 18:19:25 [core.py:843] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 540.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 426.50 MiB is free. Including non-PyTorch memory, this process has 39.07 GiB memory in use. Of the allocated memory 38.33 GiB is allocated by PyTorch, and 243.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
