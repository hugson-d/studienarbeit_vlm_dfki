Job 2366665: Running on node(s) serv-3315
Job 2366665: Started at 2025-12-06 18:14:08+0100
Monitor this job here: http://monitoring.pegasus.kl.dfki.de/d/slurm-job-details/job-details?var-jobid=2366665&from=1765041248000
srun: jobinfo: version v1.0.0
Job 2366665: Running on node(s) serv-3315
Job 2366665: Started at 2025-12-06 18:14:09+0100
Monitor this job here: http://monitoring.pegasus.kl.dfki.de/d/slurm-job-details/job-details?var-jobid=2366665&from=1765041249000
Job 2366665: creating container for /enroot/nvcr.io_nvidia_pytorch_23.12-py3.sqsh
Job 2366665: creating container for /enroot/nvcr.io_nvidia_pytorch_23.12-py3.sqsh took 20.9 seconds
Error: [Errno 13] Permission denied: '/netscratch/root'
/usr/bin/bash: line 12: /netscratch/root/.venv/vllm_internvl/bin/activate: No such file or directory
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
transformer-engine 1.1.0+cf6fc89 requires flash-attn!=2.0.9,!=2.1.0,<=2.3.3,>=1.0.6, which is not installed.
contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.2.6 which is incompatible.
cudf 23.10.0 requires numba<0.58,>=0.57, but you have numba 0.61.2 which is incompatible.
cudf 23.10.0 requires numpy<1.25,>=1.21, but you have numpy 2.2.6 which is incompatible.
dask-cudf 23.10.0 requires numpy<1.25,>=1.21, but you have numpy 2.2.6 which is incompatible.
matplotlib 3.8.2 requires numpy<2,>=1.21, but you have numpy 2.2.6 which is incompatible.
spacy 3.7.2 requires typer<0.10.0,>=0.3.0, but you have typer 0.20.0 which is incompatible.
torch-tensorrt 2.2.0a0 requires torch<2.3.0,>=2.1.0.dev, but you have torch 2.9.0 which is incompatible.
torchtext 0.17.0a0 requires torch==2.2.0a0+81ea7a4, but you have torch 2.9.0 which is incompatible.
weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.20.0 which is incompatible.
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
cudf 23.10.0 requires numba<0.58,>=0.57, but you have numba 0.61.2 which is incompatible.
cudf 23.10.0 requires numpy<1.25,>=1.21, but you have numpy 1.26.4 which is incompatible.
dask-cudf 23.10.0 requires numpy<1.25,>=1.21, but you have numpy 1.26.4 which is incompatible.
opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= "3.9", but you have numpy 1.26.4 which is incompatible.
spacy 3.7.2 requires typer<0.10.0,>=0.3.0, but you have typer 0.20.0 which is incompatible.
torch-tensorrt 2.2.0a0 requires torch<2.3.0,>=2.1.0.dev, but you have torch 2.9.0 which is incompatible.
torchtext 0.17.0a0 requires torch==2.2.0a0+81ea7a4, but you have torch 2.9.0 which is incompatible.
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.
[2025-12-06 18:18:52] INFO run_internvl3_38b_vllm.py:347: üìÇ Dataset geladen: 3557 Aufgaben
[2025-12-06 18:18:52] INFO run_internvl3_38b_vllm.py:383: üöÄ Starte InternVL3-38B-vLLM: 3557/3557 Tasks ausstehend
[2025-12-06 18:18:52] INFO run_internvl3_38b_vllm.py:228: üèóÔ∏è Lade InternVL3-38B-vLLM (38B) mit vLLM
[2025-12-06 18:18:52] INFO run_internvl3_38b_vllm.py:229:    HuggingFace ID: OpenGVLab/InternVL3-38B
[2025-12-06 18:18:52] INFO run_internvl3_38b_vllm.py:230:    ‚ö° Guided Decoding (JSON Schema) aktiviert
[2025-12-06 18:18:52] INFO run_internvl3_38b_vllm.py:233:    üì• Lade Modell mit vLLM...
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2025-12-06 18:18:54] INFO configuration_internvl_chat.py:69: vision_select_layer: -1
[2025-12-06 18:18:54] INFO configuration_internvl_chat.py:70: ps_version: v2
[2025-12-06 18:18:54] INFO configuration_internvl_chat.py:71: min_dynamic_patch: 1
[2025-12-06 18:18:54] INFO configuration_internvl_chat.py:72: max_dynamic_patch: 12
[0;36m(EngineCore_DP0 pid=708653)[0;0m [2025-12-06 18:19:05] INFO _optional_torch_c_dlpack.py:119: JIT-compiling torch-c-dlpack-ext to cache...
[0;36m(EngineCore_DP0 pid=708653)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=708653)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=708653)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/lib/python3.10/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=708653)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 847, in run_engine_core
[0;36m(EngineCore_DP0 pid=708653)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 834, in run_engine_core
[0;36m(EngineCore_DP0 pid=708653)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 610, in __init__
[0;36m(EngineCore_DP0 pid=708653)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=708653)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=708653)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[0;36m(EngineCore_DP0 pid=708653)[0;0m     self.driver_worker.load_model()
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/worker/gpu_worker.py", line 273, in load_model
[0;36m(EngineCore_DP0 pid=708653)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 3484, in load_model
[0;36m(EngineCore_DP0 pid=708653)[0;0m     self.model = model_loader.load_model(
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[0;36m(EngineCore_DP0 pid=708653)[0;0m     model = initialize_model(
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=708653)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/internvl.py", line 1120, in __init__
[0;36m(EngineCore_DP0 pid=708653)[0;0m     self.language_model = init_vllm_registered_model(
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py", line 359, in init_vllm_registered_model
[0;36m(EngineCore_DP0 pid=708653)[0;0m     return initialize_model(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=708653)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py", line 512, in __init__
[0;36m(EngineCore_DP0 pid=708653)[0;0m     self.model = Qwen2Model(
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/compilation/decorators.py", line 291, in __init__
[0;36m(EngineCore_DP0 pid=708653)[0;0m     old_init(self, **kwargs)
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py", line 365, in __init__
[0;36m(EngineCore_DP0 pid=708653)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py", line 605, in make_layers
[0;36m(EngineCore_DP0 pid=708653)[0;0m     + [
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py", line 606, in <listcomp>
[0;36m(EngineCore_DP0 pid=708653)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py", line 367, in <lambda>
[0;36m(EngineCore_DP0 pid=708653)[0;0m     lambda prefix: decoder_layer_type(
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py", line 243, in __init__
[0;36m(EngineCore_DP0 pid=708653)[0;0m     self.mlp = Qwen2MLP(
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py", line 85, in __init__
[0;36m(EngineCore_DP0 pid=708653)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[0;36m(EngineCore_DP0 pid=708653)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[0;36m(EngineCore_DP0 pid=708653)[0;0m     self.quant_method.create_weights(
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[0;36m(EngineCore_DP0 pid=708653)[0;0m     data=torch.empty(
[0;36m(EngineCore_DP0 pid=708653)[0;0m   File "/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
[0;36m(EngineCore_DP0 pid=708653)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=708653)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 540.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 426.50 MiB is free. Including non-PyTorch memory, this process has 39.07 GiB memory in use. Of the allocated memory 38.33 GiB is allocated by PyTorch, and 243.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/netscratch/dhug/studienarbeit_vlm_dfki/src/eval/vllm_models/run_internvl3_38b_vllm.py", line 496, in <module>
    run_benchmark()
  File "/netscratch/dhug/studienarbeit_vlm_dfki/src/eval/vllm_models/run_internvl3_38b_vllm.py", line 385, in run_benchmark
    evaluator = VLMEvaluator()
  File "/netscratch/dhug/studienarbeit_vlm_dfki/src/eval/vllm_models/run_internvl3_38b_vllm.py", line 234, in __init__
    self.llm = LLM(
  File "/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py", line 334, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
  File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
  File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
  File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
  File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core_client.py", line 642, in __init__
    super().__init__(
  File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core_client.py", line 471, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
  File "/usr/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
srun: error: serv-3315: task 0: Exited with exit code 1
