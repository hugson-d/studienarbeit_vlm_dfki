‚úÖ HF_TOKEN geladen
==========================================
üöÄ VLM Benchmark: Qwen2.5-VL-72B (vLLM + JSON Schema Guided Decoding)
PROJECT_ROOT: /netscratch/dhug/studienarbeit_vlm_dfki
==========================================
üì¶ Erstelle venv und installiere vLLM Dependencies...
‚úÖ Venv erstellt: /netscratch/root/.venv/vllm_qwen
Looking in indexes: http://pypi-cache/index, https://pypi.ngc.nvidia.com
Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.1)
Collecting pip
  Downloading http://pypi-cache/packages/44/3c/d717024885424591d5376220b5e836c2d5293ce2011523c9de23ff7bf068/pip-25.3-py3-none-any.whl.metadata (4.7 kB)
Downloading http://pypi-cache/packages/44/3c/d717024885424591d5376220b5e836c2d5293ce2011523c9de23ff7bf068/pip-25.3-py3-none-any.whl (1.8 MB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.8/1.8 MB 469.6 MB/s eta 0:00:00
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 23.3.1
    Uninstalling pip-23.3.1:
      Successfully uninstalled pip-23.3.1
Successfully installed pip-25.3
‚úÖ Installation abgeschlossen
DEBUG: Python: /usr/bin/python
vLLM Version: 0.12.0
Transformers: 4.57.3
Pydantic: 2.12.5
xgrammar: verf√ºgbar
‚úÖ .env geladen aus: /netscratch/dhug/studienarbeit_vlm_dfki/.env
‚úÖ HuggingFace Login erfolgreich
‚ÑπÔ∏è GuidedDecodingParams nicht verf√ºgbar - nutze guided_json direkt
INFO 12-06 17:43:40 [utils.py:253] non-default args: {'trust_remote_code': True, 'dtype': 'bfloat16', 'seed': None, 'max_model_len': 4096, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-VL-72B-Instruct'}
WARNING 12-06 17:43:40 [arg_utils.py:1175] `seed=None` is equivalent to `seed=0` in V1 Engine. You will no longer be allowed to pass `None` in v0.13.
INFO 12-06 17:43:46 [model.py:637] Resolved architecture: Qwen2_5_VLForConditionalGeneration
INFO 12-06 17:43:46 [model.py:1750] Using max model len 4096
INFO 12-06 17:43:46 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=16384.
[0;36m(EngineCore_DP0 pid=3868598)[0;0m INFO 12-06 17:43:48 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='Qwen/Qwen2.5-VL-72B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-VL-72B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=Qwen/Qwen2.5-VL-72B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=3868598)[0;0m INFO 12-06 17:43:49 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.33.226:36647 backend=nccl
[0;36m(EngineCore_DP0 pid=3868598)[0;0m INFO 12-06 17:43:49 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=3868598)[0;0m INFO 12-06 17:43:55 [gpu_model_runner.py:3467] Starting to load model Qwen/Qwen2.5-VL-72B-Instruct...
[0;36m(EngineCore_DP0 pid=3868598)[0;0m INFO 12-06 17:44:10 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=3868598)[0;0m INFO 12-06 17:50:04 [default_loader.py:308] Loading weights took 353.40 seconds
[0;36m(EngineCore_DP0 pid=3868598)[0;0m INFO 12-06 17:50:05 [gpu_model_runner.py:3549] Model loading took 136.8622 GiB memory and 368.438270 seconds
[0;36m(EngineCore_DP0 pid=3868598)[0;0m INFO 12-06 17:50:05 [gpu_model_runner.py:4306] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 834, in run_engine_core
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 610, in __init__
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     super().__init__(
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 235, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/serial_utils.py", line 479, in run_method
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 4322, in profile_run
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     dummy_encoder_outputs = self.model.embed_multimodal(
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2_5_vl.py", line 1528, in embed_multimodal
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     image_embeddings = self._process_image_input(multimodal_input)
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2_5_vl.py", line 1307, in _process_image_input
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     image_embeds = self.visual(pixel_values, grid_thw=grid_thw_list)
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2_5_vl.py", line 902, in forward
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     hidden_states = blk(
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/compilation/decorators.py", line 360, in __call__
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     return self.forward(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2_5_vl.py", line 492, in forward
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     x = residual + self.mlp(x_fused_norm)
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2_5_vl.py", line 295, in forward
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     x = self.act_fn(gate_up)
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/custom_op.py", line 46, in forward
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     return self._forward_method(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/activation.py", line 87, in forward_native
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     return F.silu(x[..., :d]) * x[..., d:]
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py", line 2371, in silu
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843]     return torch._C._nn.silu(input)
[0;36m(EngineCore_DP0 pid=3868598)[0;0m ERROR 12-06 17:50:17 [core.py:843] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacity of 139.80 GiB of which 167.25 MiB is free. Including non-PyTorch memory, this process has 139.63 GiB memory in use. Of the allocated memory 138.78 GiB is allocated by PyTorch, and 122.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
