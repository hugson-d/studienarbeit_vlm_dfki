# KÃ¤nguru-Wettbewerb VLM Dataset

Structured dataset and tooling for evaluating Vision Language Models (VLMs) on German KÃ¤nguru math competition tasks (1998-2025).

## ğŸ“Š Dataset Overview

- **3,557 tasks** ready for VLM evaluation (`dataset_final/`)
- **235 excluded tasks** (visual/quality issues) in `dataset_final_not_used/`
- **28 years** of competition data (1998-2025)
- **5 grade levels**: 3-4, 5-6, 7-8, 9-10, 11-13
- **3 difficulty levels**: A (easy), B (medium), C (hard) - balanced at ~33% each

See [DATASET_STATS.md](DATASET_STATS.md) for detailed statistics (auto-generated).

## ğŸ—‚ï¸ Dataset Structure

```json
{
  "image_path": "dataset_final/2024_7und8_B5.png",
  "year": 2024,
  "class": "7und8",
  "task_id": "B5",
  "answer": "C",
  "math_category": "Geometrie",
  "is_text_only": false,
  "extracted_text": {
    "question": "...",
    "answer_options": ["(A) ...", "(B) ...", ...]
  }
}
```

## ğŸš€ Prerequisites

- [uv](https://docs.astral.sh/uv/) (>=0.5) for Python dependency management
- Python 3.11+
- OpenAI API key (for categorization and text extraction)

Install uv on macOS:
```bash
brew install uv
```

## âš™ï¸ Setup

1. **Install dependencies:**
   ```bash
   uv sync
   ```

2. **Configure OpenAI API key:**
   ```bash
   cp .env.example .env
   # Edit .env and add your OPENAI_API_KEY
   ```

3. **Activate environment** (optional):
   ```bash
   source .venv/bin/activate
   ```

## ğŸ”§ Available Scripts

### Dataset Analysis

**Analyze dataset distribution** (generates DATASET_STATS.md):
```bash
uv run python src/analyze_dataset_distribution.py
```

Shows distribution by year, class, difficulty and generates markdown statistics.

### Utility Scripts

**Create main dataset JSON:**
```bash
uv run python src/create_dataset_json.py
```
Combines task images with solutions and generates `dataset_final.json`.

**Extract tasks from PDFs:**
```bash
# For 2012-2025 (direct extraction)
uv run python src/extract_tasks_2012_2025.py

# For 1998-2011 (OCR-based extraction)
uv run python src/extract_tasks_1998_2011.py
```

## ğŸ“‹ Task ID Format

### 2012-2025 (ABC Format)
- Files: `YYYY_class_A1.png` to `YYYY_class_C10.png`
- Task IDs: `A1-A10`, `B1-B10`, `C1-C10` (or A1-A8, B1-B8, C1-C8 for grades 3-4, 5-6)

### 1998-2011 (Converted to ABC)
- Files: `YYYY_class_1.png` to `YYYY_class_30.png` (numeric)
- Task IDs: **Converted** to ABC format in JSON for consistency

See [MAPPING_LOGIC.md](MAPPING_LOGIC.md) for detailed conversion rules.

## ğŸ“ Difficulty Mapping (1998-2011)

**Grades 3-4 and 5-6:**
- A (Easy): Tasks 1-8 â†’ A1-A8
- B (Medium): Tasks 9-16 â†’ B1-B8
- C (Hard): Tasks 17-24 â†’ C1-C8

**Grades 7-8, 9-10, 11-13:**
- A (Easy): Tasks 1-10 â†’ A1-A10
- B (Medium): Tasks 11-20 â†’ B1-B10
- C (Hard): Tasks 21-30 â†’ C1-C10

**Note:** 1998 uses "Punkte-Fragen" format instead of "Punkte-Aufgaben" (different terminology).

## ğŸ”„ Data Extraction

The dataset was built using multiple extraction methods:

1. **2012-2025:** Direct PDF extraction with PyMuPDF
2. **1998-2011:** OCR-based extraction (PDFs have encoding issues)
   - Uses Tesseract OCR with German language support
   - Marker detection: "3-Punkte-Fragen" (1998) or "3-Punkte-Aufgaben" (2000+)
   - Special case: 1998 Grade 3-4 starts with "6-Punkte-Fragen"

## ğŸ—ƒï¸ Repository Structure

```
â”œâ”€â”€ dataset_final.json          # Main dataset (3557 tasks)
â”œâ”€â”€ DATASET_STATS.md            # Auto-generated statistics
â”œâ”€â”€ MAPPING_LOGIC.md            # Task ID conversion documentation
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset_final/          # Task images (for evaluation)
â”‚   â”œâ”€â”€ dataset_final_not_used/ # Excluded task images (235)
â”‚   â”œâ”€â”€ kanguru_pdfs/           # Processed PDF files (1998-2009)
â”‚   â”œâ”€â”€ lÃ¶sungen_1998_2011.json # Solutions 1998-2011 (sorted by year)
â”‚   â””â”€â”€ lÃ¶sungen_2012_2025.json # Solutions 2012-2025
â””â”€â”€ src/
    â”œâ”€â”€ analyze_dataset_distribution.py
    â”œâ”€â”€ categorize_math_tasks.py
    â”œâ”€â”€ analyze_text_only.py
    â”œâ”€â”€ extract_text.py
    â”œâ”€â”€ extract_tasks_1998_2011.py  # OCR-based extraction
    â”œâ”€â”€ extract_tasks_2012_2025.py  # Direct PDF extraction
    â”œâ”€â”€ create_dataset_json.py      # Dataset builder
    â””â”€â”€ create_solutions_*.py       # Solution file generators
```

## ğŸ¯ Dataset Quality

- **93.8%** usable rate (3,557 out of 3,792 extracted tasks)
- Quality filtering removes tasks with:
  - Visual artifacts or poor scan quality
  - Complex multi-page layouts
  - OCR detection failures
- See detailed statistics in [DATASET_STATS.md](DATASET_STATS.md)

## ğŸ“š Documentation

- [DATASET_STATS.md](DATASET_STATS.md) - Detailed statistics and distributions
- [MAPPING_LOGIC.md](MAPPING_LOGIC.md) - Task ID format and conversion rules

## ğŸ¤– VLM Evaluation

This dataset is designed for evaluating Vision Language Models on:
- Mathematical reasoning across 28 years (1998-2025)
- Visual understanding (diagrams, graphs, geometric figures)
- German language comprehension
- Multi-choice question answering (5 options: A-E)
- Age-appropriate difficulty levels (grades 3-13)

All tasks include ground truth answers for automated evaluation.

## ğŸ” Technical Notes

### PDF Extraction Challenges (1998-2011)
- **Encoding issues:** PDFs have non-standard character encoding
- **Solution:** OCR-based extraction using Tesseract with German language support
- **Marker detection:** Different terminology between years:
  - 1998: "3-Punkte-Fragen" or "6-Punkte-Fragen" (Grade 3-4)
  - 2000-2011: "3-Punkte-Aufgaben"
- **Known issues:** Some tasks missing due to OCR detection failures

### Dataset Completeness
See [FEHLENDE_LÃ–SUNGEN.md](FEHLENDE_LÃ–SUNGEN.md) and [FEHLENDE_AUFGABEN_1998_2011.md](FEHLENDE_AUFGABEN_1998_2011.md) for documentation of missing data.
